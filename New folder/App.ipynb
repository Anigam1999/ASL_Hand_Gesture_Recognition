{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "\n",
    "# Define input shape to match input data (32, 64, 3)\n",
    "input_shape = (32, 64, 3)\n",
    "\n",
    "# Define input layer with input shape\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Build the rest of the model\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_dir = \"Data/training_set\"\n",
    "uniq_labels = sorted(os.listdir(train_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can save the images of uniform size \n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Initialize mediapipe hands module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "# Initialize mediapipe drawing module\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "model = load_model('model_vgg16.h5') \n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Unable to capture video\")\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # cv2.imshow(\"Initial Hand Landmarks\", frame)\n",
    "    # Convert BGR image to RGB before processing\n",
    "    # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect hands\n",
    "    results = hands.process(frame)\n",
    "    #for initial image \n",
    "    image = frame.copy()\n",
    "    image[:] = (255, 255, 255)\n",
    "    y_min=0\n",
    "    x_min=0\n",
    "    # Draw landmarks on the image\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw hand landmarks on the image\n",
    "            mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Get bounding box of the hand\n",
    "            x_min = int(min(hand_landmarks.landmark, key=lambda x: x.x).x * image.shape[1])-20\n",
    "            y_min = int(min(hand_landmarks.landmark, key=lambda x: x.y).y * image.shape[0])-20\n",
    "            x_max = int(max(hand_landmarks.landmark, key=lambda x: x.x).x * image.shape[1])+20\n",
    "            y_max = int(max(hand_landmarks.landmark, key=lambda x: x.y).y * image.shape[0])+20\n",
    "\n",
    "            # Draw rectangle around the hand\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "    \n",
    "    p_img = 'X'\n",
    "    if y_min>0 and x_min>0:#condition for error handling\n",
    "        e_img = image[y_min:y_max,x_min:x_max] #store the required image\n",
    "        e_img = cv2.resize(e_img, (64, 64))\n",
    "        e_img = e_img / 255.0\n",
    "        e_img = e_img.reshape((1, 64, 64, 3))\n",
    "        p_img = model.predict(e_img)\n",
    "        p_img = np.argmax(p_img, axis=1)\n",
    "        \n",
    "        # Add text box to the image\n",
    "        text = uniq_labels[p_img[0]] #predicted alphabet to be kept\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 1\n",
    "        thickness = 2\n",
    "        color = (0, 255, 0)\n",
    "        x, y = x_min, y_min # coordinates of the top-left corner of the text box\n",
    "        cv2.putText(frame, text, (x, y), font, font_scale, color, thickness)\n",
    "    cv2.imshow(\"Original Hand Landmarks\", frame)\n",
    "    # Wait for key press and exit if 'q' is pressed\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
